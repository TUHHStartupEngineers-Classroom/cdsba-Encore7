[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Assignment\n\nLoading and Overview of the Table:\n\n# Loading file random_vars:\ndf_random_vars &lt;- readRDS(\"../../Causal_Data_Science_Data/random_vars.rds\")\ndf_random_vars\n\n\n\n  \n\n\n\n\nTask 1:\nFor each variable: age and income, computing the following values:\n\nExpected Value\n\n\n# Expected Values:\nexpected_value &lt;- sapply(df_random_vars, mean)\n# Displaying the Expected Values\nexpected_values_df &lt;- data.frame(\"Expected Value of Age\" = expected_value[\"age\"], \"Expected value of Income\" = expected_value[\"income\"], row.names = '')\ncolnames(expected_values_df) &lt;- gsub(\"\\\\.\", \" \", colnames(expected_values_df))\nexpected_values_df\n\n\n\n  \n\n\n\n\nVariance\n\n\n# Variance\nvariance &lt;- sapply(df_random_vars, var)\n# Displaying the Variances\nvariances_df &lt;- data.frame(\"Variance of Age\" = variance[\"age\"], \"Variance of Income\" = variance[\"income\"], row.names = '')\ncolnames(variances_df) &lt;- gsub(\"\\\\.\", \" \", colnames(variances_df))\nvariances_df\n\n\n\n  \n\n\n\n\nStandard Deviation\n\n\n# Standard Deviation\nstd &lt;- sapply(df_random_vars, sd)\nstd_df &lt;- data.frame(\"Standard Deviation of Age\" = std[\"age\"], \"Standard Deviation of Income\" = std[\"income\"], row.names = '')\ncolnames(std_df) &lt;- gsub(\"\\\\.\", \" \", colnames(std_df))\nstd_df\n\n\n\n  \n\n\n\n\n\nTask 2:\nNo, it doesn’t makes sense to compare the standard deviations of age and income directly as this may not provide meaningful insights, as they are measured in different units. These two measures reflect the variability within each of the variable i.e age and income, but it do not offer a direct comparison between the two variables.\n\n\nTask 3:\nThe relationship between both variables with covariance and correlation:\n\nCovariance\n\n\ncovariance &lt;- cov(df_random_vars$income, df_random_vars$age)\npaste(\"Covariance is\",covariance)\n\n#&gt; [1] \"Covariance is 29700.1468458458\"\n\n\nThe covariance of 29700.15 between income and age suggests a positive relationship, indicating that as age increases, there tends to be an increase in income, and vice versa.\n\nCorrelation\n\n\ncorrelation &lt;- cor(df_random_vars$income, df_random_vars$age)\npaste(\"Correlation is\", correlation)\n\n#&gt; [1] \"Correlation is 0.547943162326476\"\n\n\nThe correlation coefficient of 0.55 between income and age suggests a moderate positive linear relationship, indicating that, on average, as age increases, there is a tendency for income to also increase.\n\n\nTask 4:\nBoth correlation and covariance are measures of the relationship between two variables, but they differ in scale and interpretability.\n\nCovariance:\n\nInterpretation: Covariance measures the extent to which two variables change together. It can be positive, negative, or zero.\nScale: The scale of covariance is in the units of the product of the units of the two variables.\nInterpretation Challenge: The challenge with interpreting covariance lies in its scale. The magnitude of the covariance is not standardized, making it difficult to compare the strength of relationships between different pairs of variables.\n\nCorrelation:\n\nInterpretation: Correlation, on the other hand, standardizes the measure of association, the value ranges between -1 and 1. A correlation of 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\nScale: The scale of correlation is standardized, making it easier to compare and interpret across different variable pairs.\nInterpretation Advantage: Correlation is more interpretable than covariance due to its standardized scale. To evaluate the strength and direction of relationships between various pairs of variables.\n\n\n\n\n\n\n\n\nConclusion\n\n\n\nIn most cases, correlation is easier to interpret and compare than covariance because it provides a standardized measure that ranges from -1 to 1. This allows a straight understanding of the strength and direction of the relationship between two variables. If the goal is to evaluate the degree and direction of association while avoiding scale-related challenges, correlation is generally preferred over covariance.\n\n\n\nThe conditional expected values:\n\n\n\\(E[income|age&lt;=18]\\):\n\n\n#The expected value of the income when age&lt;=18:\ncev_income_age_leq_18&lt;- mean(subset(df_random_vars, age &lt;= 18)$income)\npaste(\"Conditional expected value of income for age &lt;= 18:\", cev_income_age_leq_18)\n\n#&gt; [1] \"Conditional expected value of income for age &lt;= 18: 389.607438016529\"\n\n\n\n\\(E[income|age\\in[18,65)]\\)\n\n\n#The expected value of the income when 18&lt;=age&lt;65:\ncev_income_age_bw_18_65&lt;- mean(subset(df_random_vars, age %in% 18:64)$income)\npaste(\"Conditional expected value of income for age between 18 to 64:\", cev_income_age_bw_18_65)\n\n#&gt; [1] \"Conditional expected value of income for age between 18 to 64: 4685.73426573427\"\n\n\n\n\\(E[income|age&gt;=65]\\)\n\n\n#The expected value of the income when age&gt;=65:\ncev_income_age_geq_65&lt;- mean(subset(df_random_vars, age &gt;= 65)$income) \npaste(\"Conditional expected value of income for age &gt;= 65:\", cev_income_age_geq_65)\n\n#&gt; [1] \"Conditional expected value of income for age &gt;= 65: 1777.23728813559\""
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Assignment\n\n\nExample of number of facebook friends with average grade in university course\n\nGenerating data\n\n\n# Generating few data points\nstudents &lt;- seq(1, 100, by = 1)\n\nfacebook_friends &lt;- rnorm(length(students), mean = 300, sd = 100) + \n  rnorm(length(students), mean = 0, sd = 30)  # noise\n\naverage_grade &lt;- rnorm(length(students), mean = 70, sd = 10) + \n  0.5 * facebook_friends + \n  rnorm(length(students), mean = 0, sd = 10)  # noise\n\n# Creating a data frame\ndata &lt;- data.frame(Student_ID = students, \n                   Facebook_Friends = facebook_friends,\n                   Average_Grade = average_grade)\ndata\n\n\n\n  \n\n\n\n\nggplot()\n\n\n# Scatter Plot\nggplot(data, aes(x = Facebook_Friends, y = Average_Grade)) +\n  geom_point() +\n  labs(title = \"Spurious Correlation: Facebook Friends vs. Average Grade\",\n       x = \"Number of Facebook Friends\",\n       y = \"Average Grade in University Courses\")\n\n\n\n\n\n\n\n\n\nCorrelation\n\n\n# Calculate the correlation coefficient\ncorrelation_coefficient &lt;- cor(data$Facebook_Friends, data$Average_Grade)\npaste(\"Correlation coefficient: \", correlation_coefficient)\n\n#&gt; [1] \"Correlation coefficient:  0.971816354322658\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nLine Chart that shows a positive correlation between Facebook Friends and Average Grade. The correlation coefficient is &gt;0.9, which indicates a strong positive correlation.\n\n\nCorrelation doesn’t imply causation. In my generated example, I deliberately created a scenario to illustrate the concept of spurious correlation without asserting any causal link between the number of Facebook friends and academic performance.\nIn reality, there could be numerous factors influencing academic performance, and the number of Facebook friends is unlikely to be a direct cause. Some real-world factors that may contribute to good grades include:\n\nStudy Habits: Students who develop effective study habits are likely to perform better academically.\nAttendance: Regular attendance in classes and engagement with course materials can positively impact grades.\nInterest in the Subject: Genuine interest in the subject matter often leads to better understanding and performance.\nTime Management: Efficient time management is crucial for balancing academic and extracurricular activities."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Assignment\n\n\nLoading data set rand_enc\n\n\ndf_rand_enc &lt;- readRDS(\"../../Causal_Data_Science_Data/rand_enc.rds\")\ndf_rand_enc\n\n\n\n  \n\n\n\n\nTask 1:\n\ndag &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Pop-up Feature\", \n             \"Y\" = \"App Usage\", \n             \"U\" = \"Unobserved Factor\",\n             \"Z\" = \"Encouragement\")\n)\nggdag(dag, text = T) +\n  theme_dag() +\n  theme(plot.background = element_rect(fill = \"lightblue\"))\n\n\n\n\n\n\n\n\n\n\nTask 2:\n\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df_rand_enc)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df_rand_enc)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nTask 3:\n\ncor(df_rand_enc)\n\n#&gt;             rand_enc  used_ftr time_spent\n#&gt; rand_enc   1.0000000 0.2044110  0.1296716\n#&gt; used_ftr   0.2044110 1.0000000  0.7050147\n#&gt; time_spent 0.1296716 0.7050147  1.0000000\n\n\n\nCorrelation Analysis:\n\n\nThe correlation between rand_enc and used_ftr is 0.204, indicating a weak positive correlation.\nThe correlation between used_ftr and time_spent is 0.705, indicating a moderate to strong positive correlation.\nThe correlation between rand_enc and time_spent is 0.130, indicating a weak positive correlation.\n\n\nOverall Assessment:\n\n\nThe correlations between the randomly selected customer and other variables appear weak.\nThis indicates potential endogeneity issues.\nInstrumental variable estimation can be employed to mitigate bias in such cases.\nUnobserved factors may impact both the utilized features and the time spent.\nInstrumental variable estimation becomes useful in addressing omitted variable bias under these circumstances.\nThis approach helps account for unobserved variables, enhancing the robustness of the analysis.\nA valid instrumental variable for estimation should be relevant and exhibit no direct correlation with the outcome variable(time_spent), except through its association with the features.\n\n\n\nTask 4:\n\nlibrary(estimatr)\n\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df_rand_enc)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df_rand_enc)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nThe initial estimate stands at 10.82, while the Instrumental Variable (IV) estimate is marginally lower at 9.73. The initial estimate, often referred to as the naive estimate, tends to be on the higher side as it overlooks concealed factors associated with both feature usage and time spent on the app, commonly referred to as encouragement. The IV estimate rectifies this by accounting for these hidden factors, resulting in a more precise and slightly lower estimate. Unlike the naive estimate, which encompasses both the impact of feature usage on app time and the influence of encouragement, the IV estimate isolates the effect of feature usage, leading to a more accurate depiction."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Assignment\n\n\nLoading the data set membership\n\n\ndf_membership  &lt;- readRDS(\"../../Causal_Data_Science_Data/membership.rds\")\ndf_membership\n\n\n\n  \n\n\n\n\nTask 1:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\n\n# Define the DAG\ndag &lt;- 'dag {\n  card  [exposure,pos=\"0.1,0.5\"]\n  avg_purch [outcome,pos=\"0.5,0.5\"]\n  pre_avg_purch [pos=\"0.3,1\"]\n  pre_avg_purch -&gt; card\n  pre_avg_purch -&gt; avg_purch\n  card -&gt; avg_purch\n}'\nggdag_status(dag) +\ntheme_dag() +\ntheme(plot.background = element_rect(fill = \"lightblue\"))\n\n\n\n\n\n\n\n\n\n\nTask 2:\n\nmodel_naive &lt;- lm(avg_purch ~ card, data = df_membership)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_membership)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nTask 3:\n\n(Coarsened) Exact Matching.\n\n\nWithout specifying coarsening\n\nlibrary(MatchIt)\n\ncem &lt;- matchit(card ~ sex + age + pre_avg_purch, \n               data = df_membership, \n               method = \"cem\", \n               estimand = 'ATE')\n# Covariate balance\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df_membership, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5040        0.5040         -0.0000          .    0.0000\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n\nCustom coarsening\n\n\ncutpoints &lt;- list(age = seq(24, 65, 18), \n                  pre_avg_purch = seq(3, 200, 50))\ncem_coars &lt;- matchit(card ~ sex + age + pre_avg_purch, \n                     data = df_membership, \n                     method = \"cem\", \n                     estimand = 'ATE', \n                     cutpoints = cutpoints)\nsummary(cem_coars)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df_membership, \n#&gt;     method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5035        0.5035         -0.0000          .    0.0000\n#&gt; age                 40.4546       40.2862          0.0125     1.0210    0.0035\n#&gt; pre_avg_purch       71.6914       69.4008          0.0877     1.0108    0.0233\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0111          0.4227\n#&gt; pre_avg_purch   0.0533          0.5663\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5632.85 4036.73\n#&gt; Matched       5768.   4226.  \n#&gt; Unmatched        0.      6.  \n#&gt; Discarded        0.      0.\n\n\n\ndf_cem_coars &lt;- match.data(cem_coars)\n\nmodel_cem_coars &lt;- lm(avg_purch ~ card, \n                      data = df_cem, \n                      weights = weights)\nsummary(model_cem_coars)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n\nNearest-Neighbor Matching.\n\n\nnn &lt;- matchit(card ~ age + sex + pre_avg_purch, \n              data = df_membership, method = \"nearest\", \n              distance = \"mahalanobis\", replace = T)\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = df_membership, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; sex                  0.5087        0.5087         -0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n\n\ndf_nn &lt;- match.data(nn)\n\nmodel_nn &lt;- lm(avg_purch ~ card, \n               data = df_nn, \n               weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n\n\nInverse Probability Weighting.\n\n\nmodel_prop &lt;- glm(card ~ age + sex + pre_avg_purch, \n                  data = df_membership, \n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df_membership)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\ndf_aug &lt;- df_membership %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\ndf_ipw &lt;- df_aug %&gt;% mutate(ipw = (card / propensity) + ((1 - card) / (1 - propensity)))\n\ndf_ipw %&gt;% \n  select(card, age, sex, pre_avg_purch, propensity, ipw)\n\n\n\n  \n\n\n\n\nmodel_ipw &lt;- lm(avg_purch ~ card, \n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Assignment\n\n\nTask 1:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\ndag &lt;- 'dag {\n  Parking [exposure,pos=\"0.1,0.5\"]\n  Sales [outcome,pos=\"0.5,0.5\"]\n  Location [pos=\"0.3,1\"]\n  Parking -&gt; Sales\n  Location -&gt; Parking\n  Location -&gt; Sales\n}'\nggdag_status(dag) +\ntheme_dag() +\ntheme(plot.background = element_rect(fill = \"lightblue\"))\n\n\n\n\n\n\n\n\n\n\nTask 2:\n\nLoading the customer_sat data set\n\n\ndf_customer_sat &lt;- readRDS(\"../../Causal_Data_Science_Data/customer_sat.rds\")\nhead(df_customer_sat)\n\n\n\n  \n\n\n\n\n1. Regress satisfaction on follow_ups\n\nlm_model1 &lt;- lm(satisfaction ~ follow_ups, data = df_customer_sat)\n\n\nSummary\n\n\nsummary(lm_model1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = df_customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\n\n2. Regress satisfaction on follow_ups and account for subscription\n\nlm_model_2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = df_customer_sat)\n\n\nSummary\n\n\nsummary(lm_model_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = df_customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n\nTask 3:\n\nCommon Predictors:\n\nBoth models include an intercept and the predictor variable follow_ups.\nThe coefficient for follow_ups is negative in Model 1 and positive in Model 2.\n\nAdditional Predictors in Model 2:\nModel 2 includes two additional predictor variables: subscriptionPremium and subscriptionPremium+.\nThese variables have positive coefficients, indicating a positive impact on the dependent variable.\nExplanation:\n\nThe introduction of subscription-related predictors (subscriptionPremium and subscriptionPremium+) in Model 2 might be associated with a change in the business strategy.\nThe positive coefficients for subscription-related predictors in Model 2 suggest that having a premium subscription or a premium+ subscription is associated with higher values of the dependent variable.\n\nOverall Model Performance:\n\nModel 2 has a higher adjusted R-squared (0.9487) compared to Model 1 (0.6316), indicating that the predictors in Model 2 explain a larger proportion of the variance for the dependent variable.\n\nStatistical Significance:\n\nAll coefficients in Model 2, including the follow_ups, and subscription-related predictors, are statistically significant.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn summary, the differences in coefficients between the two models can be attributed to the introduction of new predictors related to subscription types in Model 2. The positive coefficients for subscription-related predictors suggest that these subscriptions contribute significantly to the higher values of the dependent variable in the presence of follow-ups.\n\n\n\n\nTask 4\n\nWithout Conditioning on Subscription\n\n\nsimps_not_cond  &lt;- ggplot(df_customer_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nsimps_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nConditioning on Subscription\n\n\nsimps_cond &lt;- ggplot(df_customer_sat, aes(x = follow_ups, y = satisfaction, color = subscription )) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\nsimps_cond \n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Assignment\n\n\nLoading the data set abtest_online\n\n\ndf_abtest_online  &lt;- readRDS(\"../../Causal_Data_Science_Data/abtest_online.rds\")\ndf_abtest_online\n\n\n\n  \n\n\n\n\nTask 1:\nPlotting the covariates to check the balance across the groups\n\nTreatment Group: chatbot = TRUE\nControl Group: chatbot = FALSE\n\n\nlibrary(tidyverse)\n\ndf_abtest_online %&gt;% \n  group_by(chatbot) %&gt;% \n  summarise(mean_outcome = mean(purchase_amount))\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe average outcome of the non-chatbot group is higher.\n\n\n\nlibrary(ggplot2)\n\ncompare_purchase_amount &lt;- \n  ggplot(df_abtest_online, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase_amount\", title = \"Difference in purchase amount\")\n\n\nPlot with outcome purchase amount\n\n\ncompare_purchase_amount\n\n\n\n\n\n\n\n\n\nThe above two plots with outcome represents covariate balance between the two groups.\n\n\n\nTask 2:\n\nlm_sales_ate &lt;- lm(purchase_amount ~ chatbot, data = df_abtest_online)\nsummary(lm_sales_ate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = df_abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\n\nThe variable chatbot appears to exert a negative influence on the purchase amount. This suggests that the presence of the chatbot has led to a decline in sales, consequently reducing the purchase amount.\n\n\n\nTask 3:\n\nSubgroup-Specific effects by including an interaction\n\n\nlm_cate1 &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = df_abtest_online)\n\nlm_cate2 &lt;- lm(purchase_amount ~ chatbot * previous_visit, data = df_abtest_online)\n\nlm_cate3 &lt;- lm(purchase_amount ~ chatbot * mobile_device + previous_visit, data = df_abtest_online)\n\nlm_cate4 &lt;- lm(purchase_amount ~ chatbot * mobile_device + chatbot * previous_visit, data = df_abtest_online)\n\n\nSummary for lm_cate4 : The R2 value for lm_cate4 is highest among above 4 interaction\n\n\nsummary(lm_cate4)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device + chatbot * \n#&gt;     previous_visit, data = df_abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -22.057 -15.928  -7.419  12.804  65.333 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    17.2052     1.2982  13.253  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -11.1064     1.8434  -6.025 2.38e-09 ***\n#&gt; mobile_deviceTRUE              -0.8791     1.7823  -0.493 0.621943    \n#&gt; previous_visit                 -0.1030     0.3749  -0.275 0.783562    \n#&gt; chatbotTRUE:mobile_deviceTRUE   0.2044     2.5148   0.081 0.935229    \n#&gt; chatbotTRUE:previous_visit      2.0978     0.5781   3.629 0.000299 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.49 on 994 degrees of freedom\n#&gt; Multiple R-squared:  0.05495,    Adjusted R-squared:  0.0502 \n#&gt; F-statistic: 11.56 on 5 and 994 DF,  p-value: 7.177e-11\n\n\n\n\nTask 4:\n\nlm_binary &lt;- glm(purchase ~ chatbot * mobile_device + chatbot * previous_visit, family = binomial(link = 'logit'), data = df_abtest_online)\nsummary(lm_binary)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot * mobile_device + chatbot * \n#&gt;     previous_visit, family = binomial(link = \"logit\"), data = df_abtest_online)\n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)                    0.03872    0.14047   0.276    0.783    \n#&gt; chatbotTRUE                   -1.65686    0.22511  -7.360 1.84e-13 ***\n#&gt; mobile_deviceTRUE             -0.05177    0.19284  -0.268    0.788    \n#&gt; previous_visit                -0.01769    0.04061  -0.436    0.663    \n#&gt; chatbotTRUE:mobile_deviceTRUE -0.01616    0.29818  -0.054    0.957    \n#&gt; chatbotTRUE:previous_visit     0.32646    0.06912   4.723 2.32e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1238.6  on 994  degrees of freedom\n#&gt; AIC: 1250.6\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n\nInterpretation of Coefficient:\nThe coefficient in the logistic regression output represents the log-odds of making a purchase when a particular variable is present compared to when it’s not. However, these coefficients are not as intuitively interpretative as in linear regression.\n\nTo interpret the coefficient:\n\nIf the coefficient is positive, it suggests an increase in the log-odds of making a purchase when the chatbot is present.\nIf the coefficient is negative, it suggests a decrease in the log-odds of making a purchase when the chatbot is present.\n\nOdds Ratio:\n\nThe exponentiated coefficient gives the odds ratio. If the odds ratio is greater than 1, it implies an increase in the odds of making a purchase. If it’s less than 1, it implies a decrease.\nFor example, if the odds ratio is 1.5, it means the odds of making a purchase are 50% higher when the chatbot is present compared to when it’s not.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn summary, the logistic regression helps understand how the presence of the variables influences the likelihood of a customer making a purchase. The coefficient and odds ratio provide insights into the direction and magnitude of this influence."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Assignment\n\n\nLoading the data set hospdd\n\n\nlibrary(dplyr)\n\ndf_hospdd  &lt;- readRDS(\"../../Causal_Data_Science_Data/hospdd.rds\")\nhead(df_hospdd)\n\n\n\n  \n\n\n\n\nHospitals in which the new admission procedure was implemented\n\n\ntreatment_hospital &lt;- df_hospdd %&gt;%\n  group_by(hospital) %&gt;%\n  summarise(procedure_count = sum(procedure))\n\ntreatment_hospital\n\n\n\n  \n\n\n\nThe above Table shows that out of 46 hospitals, hospitals with label 1 to 18 had the new admission procedure whereas the hospitals with label 19 to 46 had the old admission procedure.\n\nMonths in which the new admission procedure was implemented\n\n\ntreatment_month &lt;- df_hospdd %&gt;%\n  group_by(month) %&gt;%\n  summarise(procedure_count = sum(procedure))\n\ntreatment_month\n\n\n\n  \n\n\n\nThe above Table shows that out of 7 months, months with label 4,5,6 and 7 had the new admission procedure whereas the months with label 1,2 and 3 had the old admission procedure.\n\n\n\n\n\n\nNote\n\n\n\n\nTreatment variable = procedure\nBefore Treatment:\n\nTreatment Group: 18 hospitals before month 4\nControl Group: 28 hospitals before month 4\n\nAfter Treatment:\n\nTreatment Group: 18 hospitals from month 4\nControl Group: 28 hospitals from month 4\n\n\n\n\n\ntreatment_hospital &lt;- df_hospdd %&gt;%\n  group_by(hospital, month) %&gt;%\n  summarise(procedure_count = sum(procedure), .groups = 'drop')\n\ntreatment_hospital\n\n\n\n  \n\n\n\n\nTask 1:\n\nMean satisfaction for treated and control hospitals before and after the treatment\n\n\n# Step 1: Subset the data for treatment and control hospitals\ntreated_hospitals &lt;- df_hospdd %&gt;% filter(procedure == 1)\ncontrol_hospitals &lt;- df_hospdd %&gt;% filter(procedure == 0)\n\n# Step 2: Compute mean satisfaction for treated hospitals before and after treatment\nmean_satis_before_treated &lt;- treated_hospitals %&gt;%\n  filter(month &lt; 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\nmean_satis_before_treated = 0\n\nmean_satis_after_treated &lt;- treated_hospitals %&gt;%\n  filter(month &gt;= 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\n# Step 3: Compute mean satisfaction for control hospitals before and after treatment\nmean_satis_before_control &lt;- control_hospitals %&gt;%\n  filter(month &lt; 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\nmean_satis_after_control &lt;- control_hospitals %&gt;%\n  filter(month &gt;= 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\n# Step 4: Print the results\ncat(\"Mean Satisfaction - Before (Treated):\", mean_satis_before_treated, \"\\n\")\n\n#&gt; Mean Satisfaction - Before (Treated): 0\n\ncat(\"Mean Satisfaction - After (Treated):\", mean_satis_after_treated, \"\\n\")\n\n#&gt; Mean Satisfaction - After (Treated): 4.363351\n\ncat(\"Mean Satisfaction - Before (Control):\", mean_satis_before_control, \"\\n\")\n\n#&gt; Mean Satisfaction - Before (Control): 3.447765\n\ncat(\"Mean Satisfaction - After (Control):\", mean_satis_after_control, \"\\n\")\n\n#&gt; Mean Satisfaction - After (Control): 3.38249\n\n\n\nDiD\n\n\ndiff_before_treatment = mean_satis_before_treated - mean_satis_before_control\ndiff_after_treatment = mean_satis_after_treated - mean_satis_after_control\n\ndid_estimate &lt;- diff_after_treatment - diff_before_treatment\ndid_estimate\n\n#&gt; [1] 4.428627\n\n\n\n\nTask 2:\n\nmonth + hospital\n\n\nmodel1&lt;- lm(satis ~ month + hospital, data = df_hospdd)\nmodel1\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital, data = df_hospdd)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)        month     hospital  \n#&gt;     3.75972      0.07207     -0.01760\n\n\n\nas.factor(month) + as.factor(hospital)\n\n\nmodel2 &lt;- lm(satis ~ as.factor(month) + as.factor(hospital), data = df_hospdd)\nmodel2\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = df_hospdd)\n#&gt; \n#&gt; Coefficients:\n#&gt;           (Intercept)      as.factor(month)2      as.factor(month)3  \n#&gt;              3.419332              -0.009608               0.021969  \n#&gt;     as.factor(month)4      as.factor(month)5      as.factor(month)6  \n#&gt;              0.349354               0.343235               0.348800  \n#&gt;     as.factor(month)7   as.factor(hospital)2   as.factor(hospital)3  \n#&gt;              0.341444               0.408566               0.533625  \n#&gt;  as.factor(hospital)4   as.factor(hospital)5   as.factor(hospital)6  \n#&gt;              0.227510              -0.145353               0.447863  \n#&gt;  as.factor(hospital)7   as.factor(hospital)8   as.factor(hospital)9  \n#&gt;              1.404416               0.071876              -1.518515  \n#&gt; as.factor(hospital)10  as.factor(hospital)11  as.factor(hospital)12  \n#&gt;              1.682845               0.220965              -0.095303  \n#&gt; as.factor(hospital)13  as.factor(hospital)14  as.factor(hospital)15  \n#&gt;              0.495593               0.233043              -0.144494  \n#&gt; as.factor(hospital)16  as.factor(hospital)17  as.factor(hospital)18  \n#&gt;              1.414268               0.423543               0.153276  \n#&gt; as.factor(hospital)19  as.factor(hospital)20  as.factor(hospital)21  \n#&gt;             -1.169296              -0.376607               0.770343  \n#&gt; as.factor(hospital)22  as.factor(hospital)23  as.factor(hospital)24  \n#&gt;              0.375321               0.277726              -0.732120  \n#&gt; as.factor(hospital)25  as.factor(hospital)26  as.factor(hospital)27  \n#&gt;              0.222480              -0.209747              -0.822648  \n#&gt; as.factor(hospital)28  as.factor(hospital)29  as.factor(hospital)30  \n#&gt;              0.288001              -0.175443              -0.591916  \n#&gt; as.factor(hospital)31  as.factor(hospital)32  as.factor(hospital)33  \n#&gt;              0.088091              -0.747340              -0.877969  \n#&gt; as.factor(hospital)34  as.factor(hospital)35  as.factor(hospital)36  \n#&gt;             -0.424406              -0.069883               1.714149  \n#&gt; as.factor(hospital)37  as.factor(hospital)38  as.factor(hospital)39  \n#&gt;             -0.283590              -0.510800              -0.447491  \n#&gt; as.factor(hospital)40  as.factor(hospital)41  as.factor(hospital)42  \n#&gt;              0.697539              -0.573729               0.457143  \n#&gt; as.factor(hospital)43  as.factor(hospital)44  as.factor(hospital)45  \n#&gt;             -1.196426              -0.389582              -0.637743  \n#&gt; as.factor(hospital)46  \n#&gt;             -0.345502\n\n\n\nInterpretation:\n\n\nModel 1 assumes that month and hospital are numeric variables, and it models a linear relationship between these numeric variables and satis.\nModel 2 treats month and hospital as factors (categorical variables), and it models the categorical effects of each level of month and hospital on satis."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment 1:\n\n\nGiven that,\n\\(P(S) = 0.3\\) and \\(P(\\overline{S}) = 0.7\\)\n\\(P(T/S) = 0.2\\) and \\(P(\\overline{T}/S) = 0.8\\)\n\\(P(T/\\overline{S}) = 0.6\\) and \\(P(\\overline{T}/\\overline{S}) = 0.4\\)\n\n\nSolution\n\n\\(P(T\\cap S) = P(S) * P(T/S) = 0.3 * 0.2 = 0.06\\)\n\\(P(T\\cap \\overline{S}) = P(\\overline{S}) * P(T/\\overline{S}) = 0.7 * 0.6 = 0.42\\)\n\\(P(\\overline{T}\\cap S) = P(S) * P(\\overline{T}/S) = 0.3 * 0.8 = 0.24\\)\n\\(P(\\overline{T}\\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T}/\\overline{S}) = 0.7 * 0.4 = 0.28\\)\n\nSum of all four probabilities = 0.06 + 0.42 + 0.24 + 0.28 = 1\n\n\n\nAssignment 2:\n\n\nLet S = Smartphone, T = Tablet and C = Computer\n\n\nSolution\n\nThe percentage of customers using all three devices: \\(= P(S\\cap T\\cap C) = 0.5\\%\\)\nThe percentage of customers using at least two devices: \\(= 7.3 + 3.3 + 8.8 + 0.5 = 19.9\\%\\)\nThe percentage of customers using only one device: \\(= 42.3 + 27.8 + 10 = 80.1\\%\\)\n\n\n\n\nAssignment 3:\n\n\nGiven that,\n\\(A\\) = product is faulty vs. \\(\\overline{A}\\) = product is flawless\n\\(B\\) = alarm is triggered vs. \\(\\overline{B}\\) = no alarm\n\\(P(B|A)=0.97\\)\n\\(P(B|\\overline{A})=0.01\\)\n\\(P(A)=0.04\\)\n\n\nSolution\nBy Bayesian Theorem, \\(P(A|B) = \\frac{P(A|B)*P(A)}{P(B)} = \\frac{P(A|B)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\\)\n\n\\(P(\\overline{A}|B)=\\frac{0.01*0.96}{(0.01*0.96)+(0.97*0.04)}= 0.1983471074 =19.83\\%\\)\n\\(P(A|B)=\\frac{0.97*0.04}{(0.01*0.96)+(0.97*0.04)}= 0.8016528926 =80.17\\%\\)\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.17% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Assignment\n\n\nTask 1:\nReading the data for its dimensions,\n\n# Loading the file car_prices\ndf_car_prices &lt;- readRDS(\"../../Causal_Data_Science_Data/car_prices.rds\")\n# Dimensions of the data\ndim(df_car_prices)\n\n#&gt; [1] 181  22\n\n\nThe data has 181 rows and 22 columns.\n\n\nTask 2:\n\nDetailed look of the data\n\n\nt(summary(df_car_prices))\n\n#&gt;                                                                          \n#&gt;  aspiration      Length:181         Class :character   Mode  :character  \n#&gt;  doornumber      Length:181         Class :character   Mode  :character  \n#&gt;   carbody        Length:181         Class :character   Mode  :character  \n#&gt;  drivewheel      Length:181         Class :character   Mode  :character  \n#&gt; enginelocation   Length:181         Class :character   Mode  :character  \n#&gt;   wheelbase      Min.   : 86.60     1st Qu.: 94.50     Median : 96.50    \n#&gt;   carlength      Min.   :141.1      1st Qu.:166.3      Median :173.0     \n#&gt;    carwidth      Min.   :60.30      1st Qu.:64.00      Median :65.40     \n#&gt;   carheight      Min.   :47.80      1st Qu.:52.00      Median :53.70     \n#&gt;   curbweight     Min.   :1488       1st Qu.:2122       Median :2410      \n#&gt;  enginetype      Length:181         Class :character   Mode  :character  \n#&gt; cylindernumber   Length:181         Class :character   Mode  :character  \n#&gt;   enginesize     Min.   : 61.0      1st Qu.: 98.0      Median :120.0     \n#&gt;  fuelsystem      Length:181         Class :character   Mode  :character  \n#&gt;   boreratio      Min.   :2.540      1st Qu.:3.150      Median :3.310     \n#&gt;     stroke       Min.   :2.07       1st Qu.:3.08       Median :3.23      \n#&gt; compressionratio Min.   : 7.000     1st Qu.: 8.500     Median : 9.000    \n#&gt;   horsepower     Min.   : 48.0      1st Qu.: 70.0      Median : 95.0     \n#&gt;    peakrpm       Min.   :4200       1st Qu.:4800       Median :5200      \n#&gt;    citympg       Min.   :13.00      1st Qu.:19.00      Median :24.00     \n#&gt;   highwaympg     Min.   :16.00      1st Qu.:25.00      Median :30.00     \n#&gt;     price        Min.   : 5118      1st Qu.: 7609      Median : 9980     \n#&gt;                                                                    \n#&gt;  aspiration                                                        \n#&gt;  doornumber                                                        \n#&gt;   carbody                                                          \n#&gt;  drivewheel                                                        \n#&gt; enginelocation                                                     \n#&gt;   wheelbase      Mean   : 98.21   3rd Qu.:100.40   Max.   :120.90  \n#&gt;   carlength      Mean   :173.3    3rd Qu.:180.2    Max.   :208.1   \n#&gt;    carwidth      Mean   :65.74    3rd Qu.:66.50    Max.   :72.30   \n#&gt;   carheight      Mean   :53.58    3rd Qu.:55.50    Max.   :59.80   \n#&gt;   curbweight     Mean   :2521     3rd Qu.:2910     Max.   :4066    \n#&gt;  enginetype                                                        \n#&gt; cylindernumber                                                     \n#&gt;   enginesize     Mean   :127.1    3rd Qu.:141.0    Max.   :326.0   \n#&gt;  fuelsystem                                                        \n#&gt;   boreratio      Mean   :3.325    3rd Qu.:3.590    Max.   :3.940   \n#&gt;     stroke       Mean   :3.23     3rd Qu.:3.40     Max.   :4.17    \n#&gt; compressionratio Mean   : 8.848   3rd Qu.: 9.400   Max.   :11.500  \n#&gt;   horsepower     Mean   :106.2    3rd Qu.:116.0    Max.   :288.0   \n#&gt;    peakrpm       Mean   :5182     3rd Qu.:5500     Max.   :6600    \n#&gt;    citympg       Mean   :24.85    3rd Qu.:30.00    Max.   :49.00   \n#&gt;   highwaympg     Mean   :30.48    3rd Qu.:34.00    Max.   :54.00   \n#&gt;     price        Mean   :12999    3rd Qu.:16430    Max.   :45400\n\n\n\nCorrelation of numerical columns\n\n\n# Selecting only numeric columns\nnumeric_cols &lt;- df_car_prices[sapply(df_car_prices, is.numeric)]\n\n# Calculating correlation matrix\ncor(numeric_cols)\n\n#&gt;                    wheelbase   carlength   carwidth   carheight curbweight\n#&gt; wheelbase         1.00000000  0.86117385  0.7697984  0.54019069  0.7367123\n#&gt; carlength         0.86117385  1.00000000  0.8264703  0.44081316  0.8660053\n#&gt; carwidth          0.76979836  0.82647026  1.0000000  0.20488138  0.8480000\n#&gt; carheight         0.54019069  0.44081316  0.2048814  1.00000000  0.2141918\n#&gt; curbweight        0.73671231  0.86600528  0.8480000  0.21419176  1.0000000\n#&gt; enginesize        0.55411851  0.68017071  0.7431094 -0.02315796  0.8664640\n#&gt; boreratio         0.46436865  0.60094354  0.5515756  0.13853090  0.6382900\n#&gt; stroke            0.07182428  0.06571525  0.1143203 -0.15074728  0.1003967\n#&gt; compressionratio -0.25528942 -0.25098987 -0.2497246 -0.04784995 -0.3108351\n#&gt; horsepower        0.40436893  0.59890956  0.6971702 -0.09471443  0.8176250\n#&gt; peakrpm          -0.21992786 -0.19115857 -0.1086424 -0.15338356 -0.1596062\n#&gt; citympg          -0.58337119 -0.78122590 -0.7392601 -0.15044099 -0.8727123\n#&gt; highwaympg       -0.62686055 -0.78886402 -0.7501573 -0.18342575 -0.8873144\n#&gt; price             0.55547499  0.67307358  0.7430132  0.07452058  0.8340814\n#&gt;                   enginesize  boreratio      stroke compressionratio\n#&gt; wheelbase         0.55411851  0.4643687  0.07182428      -0.25528942\n#&gt; carlength         0.68017071  0.6009435  0.06571525      -0.25098987\n#&gt; carwidth          0.74310940  0.5515756  0.11432033      -0.24972464\n#&gt; carheight        -0.02315796  0.1385309 -0.15074728      -0.04784995\n#&gt; curbweight        0.86646404  0.6382900  0.10039673      -0.31083510\n#&gt; enginesize        1.00000000  0.5779073  0.18198111      -0.16132409\n#&gt; boreratio         0.57790729  1.0000000 -0.10077211      -0.19600019\n#&gt; stroke            0.18198111 -0.1007721  1.00000000      -0.30056815\n#&gt; compressionratio -0.16132409 -0.1960002 -0.30056815       1.00000000\n#&gt; horsepower        0.84661455  0.5917398  0.11169001      -0.22352509\n#&gt; peakrpm          -0.18477648 -0.2363817  0.06779511       0.15590249\n#&gt; citympg          -0.74373969 -0.6185363 -0.09158746       0.43789655\n#&gt; highwaympg       -0.75541425 -0.6086412 -0.07422008       0.44866322\n#&gt; price             0.88970342  0.5540692  0.03082326      -0.17646152\n#&gt;                   horsepower     peakrpm     citympg  highwaympg       price\n#&gt; wheelbase         0.40436893 -0.21992786 -0.58337119 -0.62686055  0.55547499\n#&gt; carlength         0.59890956 -0.19115857 -0.78122590 -0.78886402  0.67307358\n#&gt; carwidth          0.69717017 -0.10864244 -0.73926008 -0.75015733  0.74301316\n#&gt; carheight        -0.09471443 -0.15338356 -0.15044099 -0.18342575  0.07452058\n#&gt; curbweight        0.81762499 -0.15960618 -0.87271235 -0.88731441  0.83408139\n#&gt; enginesize        0.84661455 -0.18477648 -0.74373969 -0.75541425  0.88970342\n#&gt; boreratio         0.59173978 -0.23638175 -0.61853627 -0.60864116  0.55406923\n#&gt; stroke            0.11169001  0.06779511 -0.09158746 -0.07422008  0.03082326\n#&gt; compressionratio -0.22352509  0.15590249  0.43789655  0.44866322 -0.17646152\n#&gt; horsepower        1.00000000  0.08277528 -0.80612428 -0.77717065  0.84009827\n#&gt; peakrpm           0.08277528  1.00000000  0.02610522  0.05111792 -0.02085359\n#&gt; citympg          -0.80612428  0.02610522  1.00000000  0.97537685 -0.74198802\n#&gt; highwaympg       -0.77717065  0.05111792  0.97537685  1.00000000 -0.74079733\n#&gt; price             0.84009827 -0.02085359 -0.74198802 -0.74079733  1.00000000\n\n\n\nData types of each column and the samples\n\n\n# Data Types\nstr(df_car_prices)\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\n\nTotal unique data types in the car_price data set\n\n# Seeing the datatypes in the data\ncolumn_types &lt;- sapply(df_car_prices, class)\n# Get unique data types\nunique_types &lt;- unique(column_types)\n# Print the unique data types\nprint(unique_types)\n\n#&gt; [1] \"character\" \"numeric\"\n\n\nThere are 2 types of data types in the : character and numeric.\nThe numeric data type include both integers and decimals whereas character data types represent text or strings.\n\n\nTask 3:\n\nlm_model &lt;- lm(price ~ ., data = df_car_prices)\nsummary(lm_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = df_car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nThe significance of the features for the determination of the car price is denoted by the number of asterisks (*) in the model summary. The no. of asterisks could be interpreted as, if the statistical significance of a particular column is low, that indicate a high level of significance.\nThe columns with decent significance are:\nenginetypeohc, enginetypeohcv, cylindernumberfive, cylindernumberfour, enginesize, stroke, and peakrpm\n\n\nTask 4:\nChoice of the Regressor: enginesize\n\nData type: As observed in the linear regression summary, the enginesize variable is of numeric data type, accommodating both integer and decimal values. In this data set, it is notable for having integer values such as 130, 152, 109, etc.\nEffect on Price: The coefficient of the enginesiz is 125.934. This value indicates that for each unit increase in size, the price is expected to increase by 125.934 units, keeping other variable constants.\nStatistical Significance: The p-value for enginesize, which stands at 5 × 10^-6^, is notably small. This suggests a high level of significance, indicating a substantial effect on the Price.\n\n\n\nTask 5:\n\ndf_car_prices_upd &lt;- mutate(df_car_prices, seat_heating = TRUE)\nlm_model_sh &lt;- lm(price ~ . + seat_heating, data = df_car_prices_upd)\n\n\nSummary of the new model to see the coefficient of ‘seat_heating’\n\n\nsummary(lm_model_sh)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ . + seat_heating, data = df_car_prices_upd)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the table, it can be seen that the variable seat_heating shows no significant impact on the price. The regression model fails to establish any correlation between the price and seat_heating, resulting in a coefficient marked as NA (Not Available). This absence of correlation may be attributed to the phenomenon known as Multi-Collinearity. In this case, the seat_heating variable is consistently set to True without any variation, due to which the model is unable to calculate the coefficient for it."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Assignment\n\n\nLoading the data set coupon\n\n\nlibrary(dplyr)\n\ndf_coupon &lt;- readRDS(\"../../Causal_Data_Science_Data/coupon.rds\")\n\n# Define cut-off\nc0 &lt;- 60\n\nhead(df_coupon)\n\n\n\n  \n\n\n\n\nTask 1:\n\nSensitive change in result by running the analysis with half the bandwidth\n\n\n# Specify bandwidth\nbw &lt;- (c0 + c(-2.5, 2.5))\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df_coupon %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df_coupon %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 181   4\n\n\n\nLATE\n\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n\n#&gt; [1] \"LATE: 7.36\"\n\n\n\nPlot\n\n\nlibrary(ggplot2)\n\n# Minimum and maximum for y-axis limits\nmin_y &lt;- min(df_bw$purchase_after)\nmax_y &lt;- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw &lt;- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, linewidth = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dotted\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dotted\") +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nSensitive change in result by running the analysis with double the bandwidth\n\n\n# Specify bandwidth\nbw &lt;- (c0 + c(-10, 10))\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df_coupon %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df_coupon %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 629   4\n\n\n\nLATE\n\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n\n#&gt; [1] \"LATE: 9.51\"\n\n\n\nPlot\n\n\n# Minimum and maximum for y-axis limits\nmin_y &lt;- min(df_bw$purchase_after)\nmax_y &lt;- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw &lt;- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, linewidth = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dotted\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dotted\") +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere are the calculated Local Average Treatment Effect (LATE) values for different bandwidths:\nFor a bandwidth of 2.5 (half), the LATE is 7.36. For a bandwidth of 5, the LATE is 7.99. For a bandwidth of 10 (double), the LATE is 9.51.\n\n\n\n\n\n\nNote\n\n\n\nThis indicates LATE values are sensitive to the bandwidth chosen. The LATE is higher when the bandwidth is doubled and slightly lower when the bandwidth is halved. A larger bandwidth captures a broader range of the data, possibly introducing more variation. A smaller bandwidth might not capture enough of the treatment’s effect, leading to a smaller estimate.\n\n\n\n\nTask 2:\n\nLoading the data set shipping\n\n\ndf_shipping  &lt;- readRDS(\"../../Causal_Data_Science_Data/shipping.rds\")\nhead(df_shipping)\n\n\n\n  \n\n\n\n\nPlot\n\n\ndf_shipping &lt;- df_shipping %&gt;%\n  mutate(shipping_type = ifelse(purchase_amount &gt; 30, \"Free Shipping\", \"Shipping Charges\"))\n\nggplot(df_shipping, aes(x = purchase_amount, y = shipping_type, color = shipping_type)) +\n  geom_vline(xintercept = 30, color = palette()[2], linetype = \"dashed\") +\n  geom_point(alpha = 0.2, position = position_jitter()) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Free Shipping\", \"Shipping Charges\"))+\n  scale_color_discrete(labels = c(\"Free Shipping\", \"Shipping Charges\")) +\n  xlab(\"Purchase Amount\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nArgument\n\n\nThe presence of the dashed line at 30€ clearly defines a cut-off point. This aligns with the provided information about the free shipping offer for purchases with a total amount exceeding 30€.\nThe argument is supported by the clear visualization of the 30€ cut-off and the distribution of purchases on either side. The plot effectively illustrates how purchase_amount can serve as a running variable to distinguish between purchases eligible for free shipping and those incurring shipping charges."
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-1",
    "href": "content/01_journal/01_probability.html#assignment-1",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\) and \\(P(\\overline{S}) = 0.7\\)\n\\(P(T/S) = 0.2\\) and \\(P(\\overline{T}/S) = 0.8\\)\n\\(P(T/\\overline{S}) = 0.6\\) and \\(P(\\overline{T}/\\overline{S}) = 0.4\\)\n\n\n\n\n\\(P(T\\cap S) = P(S) * P(T/S) = 0.3 * 0.2 = 0.06\\)\n\\(P(T\\cap \\overline{S}) = P(\\overline{S}) * P(T/\\overline{S}) = 0.7 * 0.6 = 0.42\\)\n\\(P(\\overline{T}\\cap S) = P(S) * P(\\overline{T}/S) = 0.3 * 0.8 = 0.24\\)\n\\(P(\\overline{T}\\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T}/\\overline{S}) = 0.7 * 0.4 = 0.28\\)\n\nSum of all four probabilities = 0.06 + 0.42 + 0.24 + 0.28 = 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-2",
    "href": "content/01_journal/01_probability.html#assignment-2",
    "title": "Probability Theory",
    "section": "# Assignment 2:",
    "text": "# Assignment 2:\n\nLet S = Smartphone, T = Tablet and C = Computer\n\n\nSolution\n\nThe percentage of customers using all three devices: \\(= P(S\\cap T\\cap C) = 0.5\\%\\)\nThe percentage of customers using at least two devices: \\(= 7.3 + 3.3 + 8.8 + 0.5 = 19.9\\%\\)\nThe percentage of customers using only one device: \\(= 42.3 + 27.8 + 10 = 80.1\\%\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-3",
    "href": "content/01_journal/01_probability.html#assignment-3",
    "title": "Probability Theory",
    "section": "# Assignment 3:",
    "text": "# Assignment 3:\n\nGiven that,\n\\(A\\) = product is faulty vs. \\(\\overline{A}\\) = product is flawless\n\\(B\\) = alarm is triggered vs. \\(\\overline{B}\\) = no alarm\n\\(P(B|A)=0.97\\)\n\\(P(B|\\overline{A})=0.01\\)\n\\(P(A)=0.04\\)\n\n\nSolution\nBy Bayesian Theorem, \\(P(A|B) = \\frac{P(A|B)*P(A)}{P(B)} = \\frac{P(A|B)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\\)\n\n\\(P(\\overline{A}|B)=\\frac{0.01*0.96}{(0.01*0.96)+(0.97*0.04)}= 0.1983471074 =19.83\\%\\)\n\\(P(A|B)=\\frac{0.97*0.04}{(0.01*0.96)+(0.97*0.04)}= 0.8016528926 =80.17\\%\\)\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.17% that the product is faulty."
  }
]